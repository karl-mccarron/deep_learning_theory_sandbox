{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Effective Theory of Deep Linear Networks at Initialization\n",
    "\n",
    "This document will explore the claims laid out in Chapter 3 of Principles of Deep Learning Theory (it will be referred to from hereon as PDLT).\n",
    "In particular, it discusses the distribution of variables contained within deep linear networks with no bias. After some mathematical analysis, it claims the following:\n",
    "\n",
    "If the parameters in the weights &c are initialised according to Gaussian distributions, then the following should be true:\n",
    "- The first layer has a distribution which is Gaussian\n",
    "- Subsequent layers deviate from Gaussian, and this deviance has a scale that is related to the depth-to-width ratio.\n",
    "    - If this ratio if small, its deviation from Gaussianity is linear in the ratio (if we look at the 4-point connected correlator (also known as cumulant) of the preactivation distribution)\n",
    "    - If the ratio is large, its equivalent connected correlator varies exponentially from Gaussianity\n",
    "\n",
    "It also shows that in the limit of infinite width, the neuron values become Gaussian, which implies that the contributions for a neuron from those in the previous layer cancel out.\n",
    "The text claims that riding this line of complete order (where no deviance from a purely random Gaussian distribution) and chaos (where the network is so inconsistent between instantiations, and there is a lot of noise between neurons) is what defines the ability of a network to learn sufficiently complex non-linear functions.\n",
    "\n",
    "Reading the text will hopefully help to illuminate this claim; however, this notebook will attempt to verify their claim of the above using neural networks created with Pytorch. It's worth noting that their technical definition will vary from a normal attempt of creating a deep linear network in Pytorch, due to the fact that we will need to keep track of the values of the correlators (in this case, the expectations of different monomials of the network preactivations)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the deep learning model with access to preactivations for statistical analysis\n",
    "\n",
    "First, we need to define a neural network which is a deep linear model and which will also allow us to be able to read out the values of its preactivations at each layer. These are not usually easily accessible when creating a model, so we will have to edit the flow to make sure these can be accessed and saved.\n",
    "\n",
    "For now, to make the maths easier, we will set the input dimension to 10, all layers to the same width of 10, and the model depth shall be 10 as well.\n",
    "\n",
    "For the initialisation of each layer, equation (3.4) in §3 states the following choice of mean and standard deviation of the weights in the model.\n",
    "\n",
    "More specifically, for a deep linear model with max depth $L$, given the preactivations $z_i^{(\\ell)}$ for layer $\\ell$, and the weights $W_{ij}$ are such that $z_i^{(\\ell + 1)} = W_{ij}^{(\\ell + 1)} z_j^{(\\ell)}$, then the initialisation distribution for $W_{ij}^{(\\ell)}$ are as follows:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbb{E}[W_{ij}^{(\\ell)}] &= 0 \\\\\n",
    "\\mathbb{E}[W_{i_1 j_1}^{(\\ell)} W_{i_2 j_2}^{(\\ell)}] &= \\delta_{i_1 i_2} \\delta_{j_1 j_2} \\frac{C_W}{n_{\\ell - 1}}\n",
    "\\end{aligned}$$\n",
    "\n",
    "The variance is normalised as such as this factor then cancels out when integrating over the previous layer.\n",
    "\n",
    "So, we shall define the variables used in the model as follows:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "L &= 10 \\\\\n",
    "n_{\\ell} &= 10 \\quad \\forall \\, \\ell \\\\\n",
    "C_W &= 1\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, Python imports need to be laid out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DEPTH = 10\n",
    "LAYER_WIDTH = 10\n",
    "\n",
    "C_W = 1.0\n",
    "\n",
    "class DeepLinearModel(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = [\n",
    "            self.get_normal_initialised_layer()\n",
    "            for _ in range(MODEL_DEPTH)\n",
    "        ]\n",
    "\n",
    "        self.z_vals = []\n",
    "\n",
    "    def get_normal_initialised_layer() -> nn.Linear:\n",
    "        layer = nn.Linear(LAYER_WIDTH, LAYER_WIDTH, bias=False)\n",
    "        nn.init.normal_(layer, mean=0.0, std=C_W/LAYER_WIDTH)\n",
    "\n",
    "        return layer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2fbae7e460ce325830fa0fff7b3e2d348b36726d834ae18937fde9a1763159d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
