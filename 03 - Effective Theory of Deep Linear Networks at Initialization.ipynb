{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Effective Theory of Deep Linear Networks at Initialization\n",
    "\n",
    "This document will explore the claims laid out in Chapter 3 of Principles of Deep Learning Theory (it will be referred to from hereon as PDLT).\n",
    "In particular, it discusses the distribution of variables contained within deep linear networks with no bias. After some mathematical analysis, it claims the following:\n",
    "\n",
    "If the parameters in the weights &c are initialised according to Gaussian distributions, then the following should be true:\n",
    "- The first layer has a distribution which is Gaussian\n",
    "- Subsequent layers deviate from Gaussian, and this deviance has a scale that is related to the depth-to-width ratio.\n",
    "    - If this ratio if small, its deviation from Gaussianity is linear in the ratio (if we look at the 4-point connected correlator (also known as cumulant) of the preactivation distribution)\n",
    "    - If the ratio is large, its equivalent connected correlator varies exponentially from Gaussianity\n",
    "\n",
    "It also shows that in the limit of infinite width, the neuron values become Gaussian, which implies that the contributions for a neuron from those in the previous layer cancel out.\n",
    "The text claims that riding this line of complete order (where no deviance from a purely random Gaussian distribution) and chaos (where the network is so inconsistent between instantiations, and there is a lot of noise between neurons) is what defines the ability of a network to learn sufficiently complex non-linear functions.\n",
    "\n",
    "Reading the text will hopefully help to illuminate this claim; however, this notebook will attempt to verify their claim of the above using neural networks created with Pytorch. It's worth noting that their technical definition will vary from a normal attempt of creating a deep linear network in Pytorch, due to the fact that we will need to keep track of the values of the correlators (in this case, the expectations of different monomials of the network preactivations)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the deep learning model with access to preactivations for statistical analysis\n",
    "\n",
    "First, we need to define a neural network which is a deep linear model and which will also allow us to be able to read out the values of its preactivations at each layer. These are not usually easily accessible when creating a model, so we will have to edit the flow to make sure these can be accessed and saved.\n",
    "\n",
    "For now, to make the maths easier, we will set the input dimension to 10, all layers to the same width of 10, and the model depth shall be 10 as well.\n",
    "\n",
    "For the initialisation of each layer, equation (3.4) in §3 states the following choice of mean and standard deviation of the weights in the model.\n",
    "\n",
    "More specifically, for a deep linear model with max depth $L$, given the preactivations $z_i^{(\\ell)}$ for layer $\\ell$, and the weights $W_{ij}$ are such that $z_i^{(\\ell + 1)} = W_{ij}^{(\\ell + 1)} z_j^{(\\ell)}$, then the initialisation distribution for $W_{ij}^{(\\ell)}$ are as follows:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbb{E}[W_{ij}^{(\\ell)}] &= 0 \\\\\n",
    "\\mathbb{E}[W_{i_1 j_1}^{(\\ell)} W_{i_2 j_2}^{(\\ell)}] &= \\delta_{i_1 i_2} \\delta_{j_1 j_2} \\frac{C_W}{n_{\\ell - 1}}\n",
    "\\end{aligned}$$\n",
    "\n",
    "The variance is normalised as such as this factor then cancels out when integrating over the previous layer.\n",
    "\n",
    "So, we shall define the variables used in the model as follows:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "L &= 10 \\\\\n",
    "n_{\\ell} &= 10 \\quad \\forall \\, \\ell \\\\\n",
    "C_W &= 1\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, Python imports need to be laid out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "from typing import Tuple\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear\n",
    "\n",
    "MODEL_DEPTH = 10\n",
    "LAYER_WIDTH = 10\n",
    "\n",
    "C_W = 1.0\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Metadata:\n",
    "    \"\"\"\n",
    "    This is a dataclass which is to store statistical metadata about\n",
    "    preactivations in a layer.\n",
    "\n",
    "    The list of metadata collected from a layer is as follows:\n",
    "    - 2-point correlators for different nodes and same nodes\n",
    "    - 4-point correlators for different nodes and same nodes\n",
    "    \"\"\"\n",
    "    # 2-point correlators, both for separate nodes and for the same nodes\n",
    "    z11: float  # 2-point correlator, different nodes\n",
    "    z12: float  # 2-point correlator, same nodes\n",
    "\n",
    "    # 4-point correlators, both for separate nodes and for the same nodes\n",
    "    z1234: float  # 4-point correlator, 4 different nodes\n",
    "    z1123: float  # 4-point correlator, 3 different nodes\n",
    "    z1122: float  # 4-point correlator, 2 different nodes\n",
    "\n",
    "\n",
    "LayerApplicationAccumulator = Tuple[Tensor, list[Metadata]]\n",
    "\n",
    "\n",
    "def get_statistical_metadata(preactivations: Tensor) -> Metadata:\n",
    "    \"\"\"\n",
    "    Generate the metadata for preactivations.\n",
    "    Return the dataclass defined above which contains what's needed.\n",
    "\n",
    "    Args:\n",
    "        preactivations: Tensor\n",
    "            The input for a layer\n",
    "    \n",
    "    Returns:\n",
    "        Dataclass storing all of the metadata of interest.\n",
    "    \"\"\"\n",
    "    return Metadata(\n",
    "        # Mix and match which indices are used to try and minimise\n",
    "        # statistical correlation between nodes\n",
    "        z11=(preactivations[4]*preactivations[4]).item(),\n",
    "        z12=(preactivations[1]*preactivations[2]).item(),\n",
    "        z1234=(preactivations[3]*preactivations[4]*preactivations[5]*preactivations[6]).item(),\n",
    "        z1123=(preactivations[1]*preactivations[1]*preactivations[2]*preactivations[3]).item(),\n",
    "        z1122=(preactivations[4]*preactivations[4]*preactivations[7]*preactivations[7]).item(),\n",
    "    )\n",
    "\n",
    "\n",
    "def layer_fold_with_metadata(accumulator: LayerApplicationAccumulator, layer: Linear) -> LayerApplicationAccumulator:\n",
    "    layer_input, metadata = accumulator\n",
    "    layer_output = layer(layer_input)\n",
    "\n",
    "    return layer_output, [*metadata, get_statistical_metadata(layer_output)]\n",
    "\n",
    "\n",
    "class DeepLinearModel(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = [\n",
    "            self.get_normal_initialised_layer()\n",
    "            for _ in range(MODEL_DEPTH)\n",
    "        ]\n",
    "\n",
    "        self.metadata = []\n",
    "\n",
    "    @staticmethod\n",
    "    def get_normal_initialised_layer() -> nn.Linear:\n",
    "        layer = nn.Linear(LAYER_WIDTH, LAYER_WIDTH, bias=False)\n",
    "        # sqrt is added because the defined value in the literature is the variance,\n",
    "        # not the sqrt\n",
    "        nn.init.normal_(layer.weight, mean=0.0, std=np.sqrt(C_W/LAYER_WIDTH))\n",
    "\n",
    "        return layer\n",
    "    \n",
    "    def forward(self, model_input):\n",
    "        model_output, metadata = list(reduce(layer_fold_with_metadata, self.layers, (model_input, [])))\n",
    "        self.metadata = metadata\n",
    "\n",
    "        return model_output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the model defined, the next step is to randomly generate points with a model and retrieve the preactivations from it.\n",
    "The aim is then to see the generated distributions and see if their values match what was predicted theoretically, in particular in §3.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Theoretically, the distribution of the input should not matter, so\n",
    "# we will just generate a simple 10-dimensional input for these networks\n",
    "MODEL_INPUT = 3*torch.rand(LAYER_WIDTH)\n",
    "\n",
    "def generate_preactivations():\n",
    "    model = DeepLinearModel()\n",
    "    _ = model(MODEL_INPUT)\n",
    "    metadata_list = model.metadata\n",
    "\n",
    "    return np.array([\n",
    "        [\n",
    "            # 2-point correlators, both for separate nodes and for the same nodes\n",
    "            layer_data.z11,\n",
    "            layer_data.z12,\n",
    "\n",
    "            # 4-point correlators, both for separate nodes and for the same nodes\n",
    "            layer_data.z1122,\n",
    "            layer_data.z1123,\n",
    "            layer_data.z1234,\n",
    "        ]\n",
    "        for layer_data in metadata_list\n",
    "    ])\n",
    "\n",
    "\n",
    "SAMPLE_COUNT = 5_000\n",
    "\n",
    "# preallocate space in memory to speed things up\n",
    "preactivation_data = np.empty(shape=(SAMPLE_COUNT, MODEL_DEPTH, 5), dtype=float)\n",
    "with torch.no_grad():\n",
    "    for idx in range(SAMPLE_COUNT):\n",
    "        preactivation_data[idx] = generate_preactivations()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, given we now have this data, it would help data analysis if the hierarchy were changed in the indexing, ie. if the first index referred to each of the different data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "transposed_preactivations = preactivation_data.T\n",
    "\n",
    "z11_data = transposed_preactivations[0]\n",
    "z12_data = transposed_preactivations[1]\n",
    "z1122_data = transposed_preactivations[2]\n",
    "z1123_data = transposed_preactivations[3]\n",
    "z1234_data = transposed_preactivations[4]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have organised the data, it is now time to analyse it"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 1\n",
    "\n",
    "### 2-point correlators\n",
    "\n",
    "Theoretically, this is defined in equation (3.9), ie.\n",
    "\n",
    "$$\\mathbb{E}[z_i^{(1)}z_j^{(1)}] = \\delta_{ij}C_W G^{(0)}$$\n",
    "\n",
    "where $G^{(0)}$ is defined as $\\frac{1}{n_0} \\sum_{i} (x_i)^2$.\n",
    "\n",
    "So, for separate nodes ($i \\ne j$), $\\mathbb{E}[z_i^{(1)}z_j^{(1)}] = 0$.\n",
    "This corresponds to `z12_data`.\n",
    "\n",
    "On the other hand, `z11_data` refers to when $i = j$, so we will have to verify this against the experimental value of $G^{(0)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------\n",
      "Layer 1:\n",
      "i == j - expected = 8.18355, observed = 8.31534\n",
      "i != j - expected = 0.00000, observed = -0.23237\n"
     ]
    }
   ],
   "source": [
    "z12_layer_1 = z12_data[0]\n",
    "z11_layer_1 = z11_data[0]\n",
    "\n",
    "z12_layer_1_mean = np.mean(z12_layer_1)\n",
    "z11_layer_1_mean = np.mean(z11_layer_1)\n",
    "\n",
    "# In this case, n_0 is the size of the input, or the layer width.\n",
    "# G_0 here can be interpreted as the expectation of the square of the values in x\n",
    "G_0 = np.sum((MODEL_INPUT**2).numpy()) / LAYER_WIDTH\n",
    "\n",
    "expected_z11_layer_1_mean = C_W * G_0\n",
    "expected_z12_layer_1_mean = 0.0\n",
    "\n",
    "print('-------------------------------------------------------------------')\n",
    "print('Layer 1:')\n",
    "print(f'i == j - expected = {expected_z11_layer_1_mean:.5f}, observed = {z11_layer_1_mean:.5f}')\n",
    "print(f'i != j - expected = {expected_z12_layer_1_mean:.5f}, observed = {z12_layer_1_mean:.5f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2fbae7e460ce325830fa0fff7b3e2d348b36726d834ae18937fde9a1763159d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
