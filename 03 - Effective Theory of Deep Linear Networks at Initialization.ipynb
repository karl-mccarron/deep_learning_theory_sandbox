{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Â Chapter 3: Effective Theory of Deep Linear Networks at Initialization\n",
    "\n",
    "This document will explore the claims laid out in Chapter 3 of Principles of Deep Learning Theory (it will be referred to from hereon as PDLT).\n",
    "In particular, it discusses the distribution of variables contained within deep linear networks with no bias. After some mathematical analysis, it claims the following:\n",
    "If the parameters in the weights &c are initialised according to Gaussian distributions, then the following should be true:\n",
    "- The first layer has a distribution which is Gaussian\n",
    "- Subsequent layers deviate from Gaussian, and this deviance has a scale that is related to the depth-to-width ratio.\n",
    "    - If this ratio if small, its deviation from Gaussianity is linear in the ratio (if we look at the 4-point connected correlator (also known as cumulant) of the preactivation distribution)\n",
    "    - If the ratio is large, its equivalent connected correlator varies exponentially from Gaussianity\n",
    "\n",
    "It also shows that in the limit of infinite width, the neuron values become Gaussian, which implies that the contributions for a neuron from those in the previous layer cancel out.\n",
    "The text claims that riding this line of complete order (where no deviance from a purely random Gaussian distribution) and chaos (where the network is so inconsistent between instantiations, and there is a lot of noise between neurons) is what defines the ability of a network to learn sufficiently complex non-linear functions.\n",
    "\n",
    "Reading the text will hopefully help to illuminate this claim; however, this notebook will attempt to verify their claim of the above using neural networks created with Pytorch. It's worth noting that their technical definition will vary from a normal attempt of creating a deep linear network in Pytorch, due to the fact that we will need to keep track of the values of the correlators (in this case, the expectations of different monomials of the network preactivations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9 (main, Jan 11 2023, 09:18:18) [Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2fbae7e460ce325830fa0fff7b3e2d348b36726d834ae18937fde9a1763159d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
