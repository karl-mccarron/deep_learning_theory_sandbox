{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Effective Theory of Deep Linear Networks at Initialization\n",
    "\n",
    "This document will explore the claims laid out in Chapter 3 of Principles of Deep Learning Theory (it will be referred to from hereon as PDLT).\n",
    "In particular, it discusses the distribution of variables contained within deep linear networks with no bias. After some mathematical analysis, it claims the following:\n",
    "\n",
    "If the parameters in the weights &c are initialised according to Gaussian distributions, then the following should be true:\n",
    "- The first layer has a distribution which is Gaussian\n",
    "- Subsequent layers deviate from Gaussian, and this deviance has a scale that is related to the depth-to-width ratio.\n",
    "    - If this ratio if small, its deviation from Gaussianity is linear in the ratio (if we look at the 4-point connected correlator (also known as cumulant) of the preactivation distribution)\n",
    "    - If the ratio is large, its equivalent connected correlator varies exponentially from Gaussianity\n",
    "\n",
    "It also shows that in the limit of infinite width, the neuron values become Gaussian, which implies that the contributions for a neuron from those in the previous layer cancel out.\n",
    "The text claims that riding this line of complete order (where no deviance from a purely random Gaussian distribution) and chaos (where the network is so inconsistent between instantiations, and there is a lot of noise between neurons) is what defines the ability of a network to learn sufficiently complex non-linear functions.\n",
    "\n",
    "Reading the text will hopefully help to illuminate this claim; however, this notebook will attempt to verify their claim of the above using neural networks created with Pytorch. It's worth noting that their technical definition will vary from a normal attempt of creating a deep linear network in Pytorch, due to the fact that we will need to keep track of the values of the correlators (in this case, the expectations of different monomials of the network preactivations)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the deep learning model with access to preactivations for statistical analysis\n",
    "\n",
    "First, we need to define a neural network which is a deep linear model and which will also allow us to be able to read out the values of its preactivations at each layer. These are not usually easily accessible when creating a model, so we will have to edit the flow to make sure these can be accessed and saved.\n",
    "\n",
    "For now, to make the maths easier, we will set the input dimension to 10, all layers to the same width of 10, and the model depth shall be 10 as well.\n",
    "\n",
    "For the initialisation of each layer, equation (3.4) in §3 states the following choice of mean and standard deviation of the weights in the model.\n",
    "\n",
    "More specifically, for a deep linear model with max depth $L$, given the preactivations $z_i^{(\\ell)}$ for layer $\\ell$, and the weights $W_{ij}$ are such that $z_i^{(\\ell + 1)} = W_{ij}^{(\\ell + 1)} z_j^{(\\ell)}$, then the initialisation distribution for $W_{ij}^{(\\ell)}$ are as follows:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbb{E}[W_{ij}^{(\\ell)}] &= 0 \\\\\n",
    "\\mathbb{E}[W_{i_1 j_1}^{(\\ell)} W_{i_2 j_2}^{(\\ell)}] &= \\delta_{i_1 i_2} \\delta_{j_1 j_2} \\frac{C_W}{n_{\\ell - 1}}\n",
    "\\end{aligned}$$\n",
    "\n",
    "The variance is normalised as such as this factor then cancels out when integrating over the previous layer.\n",
    "\n",
    "So, we shall define the variables used in the model as follows:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "L &= 10 \\\\\n",
    "n_{\\ell} &= 10 \\quad \\forall \\, \\ell \\\\\n",
    "C_W &= 1\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, Python imports need to be laid out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "from typing import Tuple\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear\n",
    "\n",
    "MODEL_DEPTH = 10\n",
    "LAYER_WIDTH = 10\n",
    "\n",
    "C_W = 1.0\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Metadata:\n",
    "    \"\"\"\n",
    "    This is a dataclass which is to store statistical metadata about\n",
    "    preactivations in a layer.\n",
    "\n",
    "    The list of metadata collected from a layer is as follows:\n",
    "    - 2-point correlators for different nodes and same nodes\n",
    "    - 4-point correlators for different nodes and same nodes\n",
    "    \"\"\"\n",
    "    # 2-point correlators, both for separate nodes and for the same nodes\n",
    "    z11: float  # 2-point correlator, different nodes\n",
    "    z12: float  # 2-point correlator, same nodes\n",
    "\n",
    "    # 4-point correlators, both for separate nodes and for the same nodes\n",
    "    z1234: float  # 4-point correlator, 4 different nodes\n",
    "    z1123: float  # 4-point correlator, 3 different nodes\n",
    "    z1122: float  # 4-point correlator, 2 different nodes\n",
    "\n",
    "\n",
    "LayerApplicationAccumulator = Tuple[Tensor, list[Metadata]]\n",
    "\n",
    "\n",
    "def get_statistical_metadata(preactivations: Tensor) -> Metadata:\n",
    "    \"\"\"\n",
    "    Generate the metadata for preactivations.\n",
    "    Return the dataclass defined above which contains what's needed.\n",
    "\n",
    "    Args:\n",
    "        preactivations: Tensor\n",
    "            The input for a layer\n",
    "    \n",
    "    Returns:\n",
    "        Dataclass storing all of the metadata of interest.\n",
    "    \"\"\"\n",
    "    return Metadata(\n",
    "        # Mix and match which indices are used to try and minimise\n",
    "        # statistical correlation between nodes\n",
    "        z11=preactivations[0]*preactivations[0],\n",
    "        z12=preactivations[1]*preactivations[2],\n",
    "        z1234=preactivations[3]*preactivations[4]*preactivations[5]*preactivations[6],\n",
    "        z1123=preactivations[1]*preactivations[1]*preactivations[2]*preactivations[3],\n",
    "        z1122=preactivations[4]*preactivations[4]*preactivations[7]*preactivations[7],\n",
    "    )\n",
    "\n",
    "\n",
    "def layer_fold_with_metadata(accumulator: LayerApplicationAccumulator, layer: Linear) -> LayerApplicationAccumulator:\n",
    "    layer_input, metadata = accumulator\n",
    "    layer_output = layer(layer_input)\n",
    "\n",
    "    return layer_output, [*metadata, get_statistical_metadata(layer_output)]\n",
    "\n",
    "\n",
    "class DeepLinearModel(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = [\n",
    "            self.get_normal_initialised_layer()\n",
    "            for _ in range(MODEL_DEPTH)\n",
    "        ]\n",
    "\n",
    "        self.metadata = []\n",
    "\n",
    "    @staticmethod\n",
    "    def get_normal_initialised_layer() -> nn.Linear:\n",
    "        layer = nn.Linear(LAYER_WIDTH, LAYER_WIDTH, bias=False)\n",
    "        nn.init.normal_(layer, mean=0.0, std=C_W/LAYER_WIDTH)\n",
    "\n",
    "        return layer\n",
    "    \n",
    "    def forward(self, model_input):\n",
    "        model_output, metadata = list(reduce(layer_fold_with_metadata, self.layers, (model_input, [])))\n",
    "        self.metadata = metadata\n",
    "\n",
    "        return model_output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the model defined, the next step is to randomly generate points with a model and retrieve the preactivations from it.\n",
    "The aim is then to see the generated distributions and see if it follows a Gaussian form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Linear' object has no attribute 'normal_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m preactivation_data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty(shape\u001b[39m=\u001b[39m(SAMPLE_COUNT, MODEL_DEPTH, \u001b[39m5\u001b[39m), dtype\u001b[39m=\u001b[39m\u001b[39mfloat\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(SAMPLE_COUNT):\n\u001b[1;32m---> 27\u001b[0m     preactivation_data[idx] \u001b[39m=\u001b[39m generate_preactivations()\n",
      "Cell \u001b[1;32mIn[9], line 4\u001b[0m, in \u001b[0;36mgenerate_preactivations\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_preactivations\u001b[39m():\n\u001b[1;32m----> 4\u001b[0m     model \u001b[39m=\u001b[39m DeepLinearModel()\n\u001b[0;32m      5\u001b[0m     _ \u001b[39m=\u001b[39m model(MODEL_INPUT)\n\u001b[0;32m      6\u001b[0m     metadata_list \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mmetadata\n",
      "Cell \u001b[1;32mIn[8], line 70\u001b[0m, in \u001b[0;36mDeepLinearModel.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     68\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers \u001b[39m=\u001b[39m [\n\u001b[0;32m     71\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_normal_initialised_layer()\n\u001b[0;32m     72\u001b[0m         \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(MODEL_DEPTH)\n\u001b[0;32m     73\u001b[0m     ]\n\u001b[0;32m     75\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata \u001b[39m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn[8], line 71\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     68\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[0;32m     70\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers \u001b[39m=\u001b[39m [\n\u001b[1;32m---> 71\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_normal_initialised_layer()\n\u001b[0;32m     72\u001b[0m         \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(MODEL_DEPTH)\n\u001b[0;32m     73\u001b[0m     ]\n\u001b[0;32m     75\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata \u001b[39m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn[8], line 80\u001b[0m, in \u001b[0;36mDeepLinearModel.get_normal_initialised_layer\u001b[1;34m()\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m     78\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_normal_initialised_layer\u001b[39m() \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m nn\u001b[39m.\u001b[39mLinear:\n\u001b[0;32m     79\u001b[0m     layer \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(LAYER_WIDTH, LAYER_WIDTH, bias\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m---> 80\u001b[0m     nn\u001b[39m.\u001b[39;49minit\u001b[39m.\u001b[39;49mnormal_(layer, mean\u001b[39m=\u001b[39;49m\u001b[39m0.0\u001b[39;49m, std\u001b[39m=\u001b[39;49mC_W\u001b[39m/\u001b[39;49mLAYER_WIDTH)\n\u001b[0;32m     82\u001b[0m     \u001b[39mreturn\u001b[39;00m layer\n",
      "File \u001b[1;32mc:\\Users\\karlm\\miniconda3\\lib\\site-packages\\torch\\nn\\init.py:155\u001b[0m, in \u001b[0;36mnormal_\u001b[1;34m(tensor, mean, std)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39moverrides\u001b[39m.\u001b[39mhas_torch_function_variadic(tensor):\n\u001b[0;32m    154\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39moverrides\u001b[39m.\u001b[39mhandle_torch_function(normal_, (tensor,), tensor\u001b[39m=\u001b[39mtensor, mean\u001b[39m=\u001b[39mmean, std\u001b[39m=\u001b[39mstd)\n\u001b[1;32m--> 155\u001b[0m \u001b[39mreturn\u001b[39;00m _no_grad_normal_(tensor, mean, std)\n",
      "File \u001b[1;32mc:\\Users\\karlm\\miniconda3\\lib\\site-packages\\torch\\nn\\init.py:19\u001b[0m, in \u001b[0;36m_no_grad_normal_\u001b[1;34m(tensor, mean, std)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_no_grad_normal_\u001b[39m(tensor, mean, std):\n\u001b[0;32m     18\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m---> 19\u001b[0m         \u001b[39mreturn\u001b[39;00m tensor\u001b[39m.\u001b[39;49mnormal_(mean, std)\n",
      "File \u001b[1;32mc:\\Users\\karlm\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1612\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[0;32m   1613\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1614\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1615\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Linear' object has no attribute 'normal_'"
     ]
    }
   ],
   "source": [
    "MODEL_INPUT = 5*torch.rand(LAYER_WIDTH)\n",
    "\n",
    "def generate_preactivations():\n",
    "    model = DeepLinearModel()\n",
    "    _ = model(MODEL_INPUT)\n",
    "    metadata_list = model.metadata\n",
    "\n",
    "    return np.array([\n",
    "        [\n",
    "            # 2-point correlators, both for separate nodes and for the same nodes\n",
    "            layer_data.z11,\n",
    "            layer_data.z12,\n",
    "\n",
    "            # 4-point correlators, both for separate nodes and for the same nodes\n",
    "            layer_data.z1122,\n",
    "            layer_data.z1123,\n",
    "            layer_data.z1234,\n",
    "        ]\n",
    "        for layer_data in metadata_list\n",
    "    ])\n",
    "\n",
    "\n",
    "SAMPLE_COUNT = 1000\n",
    "\n",
    "preactivation_data = np.empty(shape=(SAMPLE_COUNT, MODEL_DEPTH, 5), dtype=float)\n",
    "for idx in range(SAMPLE_COUNT):\n",
    "    preactivation_data[idx] = generate_preactivations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2fbae7e460ce325830fa0fff7b3e2d348b36726d834ae18937fde9a1763159d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
